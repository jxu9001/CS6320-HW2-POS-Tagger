import os
import collections
import numpy as np
from sklearn.preprocessing import normalize

class HMMTagger:
    def __init__(self):
        self.sentences = []
        self.state_space = None  # k: POS tags, v: array index
        self.observation_space = None  # k: words in the corpus, v: array index
        self.state_to_tag = None # k: array index, v: POS tags
        self.pi = None # starting probabilities, expressed as log probs
        self.tr = None # transition probabilities, expressed as log probs
        self.em = None # emission probabilities, expressed as log probs

    def load_corpus(self, path):
        """
        Loads each sentence in the corpus into a list.
        Each element of that list is a list in the form [(word_1, tag_1), (word_2, tag_2), ..., (word_n, tag_n)].
        """
        for file_name in os.listdir(path):
            with open(os.path.join(path, file_name), 'r') as f:
                for l in f:
                    if l.strip():
                        # incomprehensible list comprehensions ftw
                        sentence = [tuple(map(str.lower, word_tag.split('/'))) for word_tag in l.split() if l.strip()]
                        self.sentences.append(sentence)

    def initialize_probabilities(self, sentences):
        """
        Given the list of sentences generated by load_corpus(), initialize the initial tag probabilities, the transition
        probabilities, and the emission probabilities.
        """
        # initialize the state space and observation space
        ss= {'eos'}
        os = set()
        for sentence in sentences:
            for (word, tag) in sentence:
                ss.add(tag)
                os.add(word)
        ss = sorted(ss)
        os = sorted(os)
        self.state_space = {tag: i for i, tag in enumerate(ss)}
        self.observation_space = {word: i for i, word in enumerate(os)}
        self.state_to_tag = {i: tag for i, tag in enumerate(ss)}

        # initialize the initial tag probabilities
        initial_tag_counts = collections.Counter(sentence[0][1] for sentence in sentences)
        self.pi = np.zeros(len(self.state_space))
        for tag, count in initial_tag_counts.items():
            self.pi[self.state_space[tag]] = count / len(sentences)
        self.pi = np.log(self.pi + 1e-16)

        # initialize the transition probabilities
        self.tr = np.zeros((len(self.state_space), len(self.state_space)))
        for sentence in sentences:
            for i in range(len(sentence) - 1):
                curr_tag = self.state_space[sentence[i][1]]
                next_tag = self.state_space[sentence[i + 1][1]] if i != len(sentence) - 1 else self.state_space['eos']
                self.tr[curr_tag, next_tag] += 1
        self.tr = normalize(self.tr, axis=1, norm='l1')
        self.tr = np.log(self.tr + 1e-16)

        # initialize the emission probabilities
        self.em = np.zeros((len(self.state_space), len(self.observation_space)))
        for sentence in sentences:
            for (word, tag) in sentence:
                word = self.observation_space[word]
                tag = self.state_space[tag]
                self.em[tag, word] += 1
        self.em = normalize(self.em, axis=1, norm='l1')
        self.em = np.log(self.em + 1e-16)




    def viterbi_decode(self, sentence):
        """
        Returns the most likely tag sequence for a given sentence using the Viterbi algorithm.
        Given the sentence 'People race tomorrow .', this method may return [‘NOUN’, ‘VERB’, ‘NOUN’, ‘PUNCTUATION’]
        """
        ob_seq = [self.observation_space[word] for word in sentence.split()]
        trellis = collections.defaultdict(float)
        backpointers = collections.defaultdict(int)
        best_path = []
        for i in self.state_space.values():
            trellis[i, 0] = self.pi[i] + self.em[i, ob_seq[0]]
            backpointers[i, 0] = 0
        for j in range(1, len(ob_seq)):
            for i in self.state_space.values():
                k = np.argmax([trellis[prev_state, j - 1] + self.tr[prev_state, i] + self.em[i, ob_seq[j]] for prev_state in self.state_space.values()])
                trellis[i, j] = trellis[k, j - 1] + self.tr[k, i] + self.em[i, ob_seq[j]]
                backpointers[i, j] = k

        k = np.argmax([trellis[final_state, len(ob_seq) - 1] for final_state in self.state_space.values()])
        for j in range(len(ob_seq) - 1, -1, -1):
            best_path.insert(0, k)
            k = backpointers[k, j]

        return [self.state_to_tag[state] for state in best_path]

