import os
import collections
import numpy as np
from sklearn.preprocessing import normalize


class HMMTagger:
    def __init__(self):
        self.sentences = []
        self.state_space = None  # k: POS tags, v: array index
        self.observation_space = None  # k: words in the corpus, v: array index
        self.idx_to_tag = None  # k: array index, v: POS tags
        self.pi = None  # starting probabilities, expressed as log probs
        self.tr = None  # transition probabilities, expressed as log probs
        self.em = None  # emission probabilities, expressed as log probs

    def load_corpus(self, path):
        """
        Loads each sentence in the corpus into a list.
        Each element of that list is a list in the form [(word_1, tag_1), (word_2, tag_2), ..., (word_n, tag_n)].
        """
        for file_name in os.listdir(path):
            with open(os.path.join(path, file_name), 'r') as f:
                for line in f:
                    if line.strip():
                        sentence = []
                        for word_tag in line.split():
                            word = word_tag.split('/')[0].lower()
                            tag = word_tag.split('/')[1]
                            sentence.append((word, tag))
                        self.sentences.append(sentence)

    def initialize_probabilities(self, sentences):
        """
        Given the list of sentences generated by load_corpus(), initialize the initial tag probabilities, the transition
        probabilities, and the emission probabilities.
        """
        # initialize the state space and observation space
        self.state_space = sorted({wt[1] for sentence in sentences for wt in sentence})
        self.observation_space = sorted({wt[0] for sentence in sentences for wt in sentence})
        self.state_space = {tag: i for i, tag in enumerate(self.state_space)}
        self.observation_space = {word: i for i, word in enumerate(self.observation_space)}
        self.idx_to_tag = {i: tag for i, tag in enumerate(self.state_space)}

        # initialize the initial tag probabilities
        initial_tag_counts = collections.Counter(sentence[0][1] for sentence in sentences)
        self.pi = np.zeros(len(self.state_space))
        for tag, count in initial_tag_counts.items():
            self.pi[self.state_space[tag]] = count / len(sentences)
        self.pi = np.log(self.pi + 1e-16)

        # initialize the transition probabilities
        self.tr = np.zeros((len(self.state_space), len(self.state_space)))
        for sentence in sentences:
            for i in range(len(sentence) - 1):
                curr_tag = self.state_space[sentence[i][1]]
                next_tag = self.state_space[sentence[i + 1][1]]
                self.tr[curr_tag, next_tag] += 1
        self.tr = normalize(self.tr, axis=1, norm='l1')
        self.tr = np.log(self.tr + 1e-16)

        # initialize the emission probabilities
        self.em = np.zeros((len(self.state_space), len(self.observation_space)))
        for sentence in sentences:
            for (word, tag) in sentence:
                word = self.observation_space[word]
                tag = self.state_space[tag]
                self.em[tag, word] += 1
        self.em = normalize(self.em, axis=1, norm='l1')
        self.em = np.log(self.em + 1e-16)

    def viterbi_decode(self, sentence):
        """
        Returns the most likely tag sequence for a given sentence using the Viterbi algorithm.
        Given the sentence 'People race tomorrow .', this method may return [‘NOUN’, ‘VERB’, ‘NOUN’, ‘PUNCTUATION’]
        """
        ob_seq = [self.observation_space[word] for word in sentence.split()]
        states = self.state_space.values()
        trellis = collections.defaultdict(float)
        back_pointers = collections.defaultdict(int)
        best_path = []

        for s in self.state_space.values():
            trellis[s, 0] = self.pi[s] + self.em[s, ob_seq[0]]
            back_pointers[s, 0] = 0

        for o in range(1, len(ob_seq)):
            for s in states:
                k = np.argmax([trellis[k, o - 1] + self.tr[k, s] + self.em[s, ob_seq[o]] for k in states])
                trellis[s, o] = trellis[k, o - 1] + self.tr[k, s] + self.em[s, ob_seq[o]]
                back_pointers[s, o] = k

        k = np.argmax([trellis[final_state, len(ob_seq) - 1] for final_state in states])
        for o in range(len(ob_seq) - 1, -1, -1):
            best_path.insert(0, self.idx_to_tag[k])
            k = back_pointers[k, o]

        return best_path
